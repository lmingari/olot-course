{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705a59d-7cb1-4e06-83c7-29c519f19472",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Install requirements\n",
    "from os.path import isfile\n",
    "\n",
    "repository   = \"https://github.com/lmingari/olot-course.git\"\n",
    "requirements = \"requirements-section2-1.txt\"\n",
    "\n",
    "if not isfile(requirements):\n",
    "    !git clone {repository}\n",
    "    %cd olot-course\n",
    "    !pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502965a2-cf49-4584-9d05-c833ba44dcf6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 2.1 Training a Neural Network with PyTorch\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ace8a-57d3-4051-b63d-0d4e2cee661c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Supervised learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5641938-e3e2-470c-a9db-30544be0e3fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "| A general workflow for supervised learning |\n",
    "| --- |\n",
    "| ![](figs/supervised.svg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6f773-e38f-4791-8fc2-4bac5f0f98e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Implementation of a neural network in PyTorch: General overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e674e9-39e7-48c5-b368-784263ec3a26",
   "metadata": {},
   "source": [
    "* __Python__ is a popular programming language that emphasizes code readability\n",
    "* __PyTorch__ is a Python-based scientific computing package for deep learning\n",
    "    \n",
    "| Programming language | Deep learning libraries |\n",
    "| ------ | ------- |\n",
    "| ![](figs/python_logo.svg) | ![](figs/pytorch_logo.svg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a42ad4-647d-4574-88fd-022a2c02569e",
   "metadata": {},
   "source": [
    "| PyTorch typical workflow |\n",
    "| --- |\n",
    "| <img src=\"figs/pytorch-workflow.svg\" width=600px> |\n",
    "\n",
    "#### Required objects\n",
    "\n",
    "We need to instantiate a few objects listed below. These objects define how the model learns from data — the __model__ predicts, the __criterion__ evaluates, the __optimizer__ improves, and the __dataloader__ keeps the data flowing:\n",
    "\n",
    "* __Model__: Defines the architecture of the neural network (layers, activations, etc.). It transforms input data into predictions\n",
    "* __DataLoader__: Provides an efficient way to feed data to the model in batches, with optional shuffling and parallel loading. It wraps around datasets for training and testing\n",
    "* __Criterion (Loss Function)__: Measures how far the model’s predictions are from the target values. It’s used to guide the learning process by computing an error signal\n",
    "* __Optimizer__: Updates the model’s parameters based on the gradients computed from the loss. It implements the learning rule (e.g., SGD, Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6025c4-1291-4381-90c2-44fec3bed28c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf4407-c944-439b-93e1-b28da362730a",
   "metadata": {},
   "source": [
    "A __perceptron__ is a basic unit in neural networks. It processes inputs with weighted connections and a bias, producing binary outputs through an activation function:\n",
    "\n",
    "| Single perceptron unit   |\n",
    "| ------------------------ |\n",
    "| <img src=\"figs/perceptron.svg\" width=400/> |\n",
    "\n",
    "| Perceptron unit output  |\n",
    "| ----------------------- |\n",
    "| $$ f(x_0, \\dots, x_n) = \\varphi \\left( \\sum_{i=0}^{n} w_i x_i \\right) $$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0627b-db0c-47c7-947d-aead85613e12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns\n",
    "a function $f: R^m \\rightarrow R^o$ by training on a dataset.\n",
    "Given a set of features $X = \\{x_1, x_2, ..., x_m\\}$ and a target $y$, \n",
    "it can learn a non-linear function approximator for either classification or regression.\n",
    "\n",
    "| Multilayer perceptron |\n",
    "| ----------------------|\n",
    "| <img src=\"figs/mlp.svg\" width=600/> |\n",
    "\n",
    "[mlp]: https://scikit-learn.org/stable/modules/neural_networks_supervised.html \"MLP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba520-e20c-492f-a733-a1a7bd9c28cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementing a MLP\n",
    "\n",
    "Let's check the installation by importing the essential pytorch modules/sub-modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a42ad3-7ab1-402b-bc3b-6e5dc1d551d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch                                               # main PyTorch library for tensor computation\n",
    "import torch.nn as nn                                      # building blocks for creating and training neural networks\n",
    "import torch.optim as optim                                # implementation of various optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader           # dataset utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a3954-17f0-4fe5-94a2-923df8a5dcab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 1. Loading raw data and feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a4459-8b92-41c1-9587-77695488e337",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to normalize the range of independent variables or features of data.\n",
    "In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n",
    "* __Normalization__: scales features to a specific range\n",
    "\n",
    "$$ X \\to \\dfrac{X - X_{min}}{X_{max} - X_{min}} $$\n",
    "\n",
    "* __Standardization__: transforms data to have a mean of 0 and a standard deviation of 1\n",
    "\n",
    "$$ X \\to \\dfrac{X-\\mu}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12f8ac-4d0f-4f25-b025-adb2a76b2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = 4\n",
    "ntargets  = 1\n",
    "nexamples = 100\n",
    "\n",
    "X = torch.rand(nexamples,nfeatures) ## Features\n",
    "y = torch.ones(nexamples,ntargets)  ## Targets\n",
    "\n",
    "print(\"Features array: \", X.shape)\n",
    "print(\"Target vector: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bfb15-bcba-4df0-883f-3d964043d1fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 2. Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77c67e-9109-45b8-b939-9078f6d35688",
   "metadata": {},
   "source": [
    "Data splitting refers to the practice of dividing a dataset into distinct subsets to facilitate the training, testing, and evaluation of machine learning models. By separating the data, we ensure that the model is trained on one set, validated on another, and tested on a final, independent set.\n",
    "\n",
    "1. __Training dataset__: Used to optimize the model parameters (learn weights)\n",
    "2. __Validation dataset__: Used during training to monitor performance (used for hyperparameter tuning, early stopping, etc...)\n",
    "3. __Test dataset__: Used once to evaluate the final chosen model for a fair (unbiased) reporting of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85315bf1-fc25-4c81-b05d-8a1ee69bf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, X, y, transform = None):\n",
    "        \"\"\"\n",
    "        X: NumPy array of shape (n_samples, n_features)\n",
    "        y: NumPy array of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cce0cd-512a-451a-97de-a6431b9d1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define out training dataset for now\n",
    "dataset = myDataset(X,y)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb116a-4edb-4c88-a2d7-0da2b3497aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the 4th data sample\n",
    "X, y = dataset[3]\n",
    "print(f'Features: {X}')\n",
    "print(f'Target: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb15ba6-0c5d-4f04-a759-78c9e3f2c072",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 3. Create DataLoader\n",
    "\n",
    "While training a model, we typically want to pass samples in \"minibatches\". `DataLoader` is an iterable that abstracts this complexity for us in an easy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906078b-8da5-42c5-9bab-f8656d093fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader with batch_size of 16\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ed578-1ed7-48ee-a869-4a825d04dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in loader:\n",
    "    print(\"**** Mini-batch of features: \\n\", xb)\n",
    "    print(\"**** Mini-batch of targets: \\n\", yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb19edb-f1c1-47c8-9ca4-ae959cc2d6e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 4. Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea920b2-ab2d-4ee8-a7a1-7200ba008751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for a multilayer perceptron with a single hidden layer\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, nfeatures=4, nhidden=16):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(nfeatures, nhidden), # input layer -> hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nhidden, 1),         # hidden layer -> output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e79a15-14e8-4e10-a87a-5b3d7d6eecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Instantiate model\n",
    "model = MultiLayerPerceptron()\n",
    "summary(model, (nfeatures,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5dfe5-9ff1-49d4-96ee-8db27f2f343c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for xb, yb in loader:\n",
    "    # Make a prediction\n",
    "    yp = model(xb)\n",
    "    print(\"**** Mini-batch of predictions: \", yp.shape)\n",
    "    print(\"**** Mini-batch of targets: \", yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021a7bf-d404-4850-8a36-e8bee0bd5fdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 5. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae862dc-d1ac-4f41-9765-518110a285da",
   "metadata": {},
   "source": [
    "Let's define a loss function based on the mean squared error:\n",
    "\n",
    "$$ L = \\dfrac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y}_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db76dbf-1499-4e04-b30a-ba7b64c2b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a criterion that measures the mean squared error\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69243789-70d9-4364-b7bd-b331f2220c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch loop\n",
    "for xb, yb in loader:\n",
    "    # Make a prediction\n",
    "    yp = model(xb)\n",
    "    # Compute MSE loss\n",
    "    loss = criterion(yp,yb)\n",
    "    print(\"**** Averaged loss: \", loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad97ae-6a0c-4a14-a49e-1b45a22b0b3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a563769-aa83-4d96-8b76-db089f474d89",
   "metadata": {},
   "source": [
    "According the _Gradient Descent (GD)_ Optimization algorithm, the weights $\\mathbf{w}$ are updated incrementally by taking a step in the opposite direction of the cost gradient:\n",
    "\n",
    "$$ \\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\dfrac{\\partial L}{\\partial \\mathbf{w}} (t) $$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "* The __learning rate__ is a key __hyperparameter__ in neural networks that controls how quickly the model learns during training\n",
    "\n",
    "| Gradient Descent optimization |\n",
    "| --- |\n",
    "| <img src=\"figs/optimization.svg\" width=600/> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e1207-bee3-4017-8bbd-b19529808094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer with learning rate 1E-3 = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=1E-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc06f8-fa5e-49a6-bd93-bad4a265d2eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A template for the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12e276-4f9e-46be-b36c-1edf5d418e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    # Set training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for xb, yb in loader:\n",
    "        # Make a prediction\n",
    "        yp = model(xb)\n",
    "        # Compute MSE loss\n",
    "        loss = criterion(yp,yb)\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update total loss\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361f38d-12a8-42c0-83ac-7298ea01a587",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e736f77-0d0d-4bc9-9f1c-30853f9aa6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPOCHS = 500\n",
    "\n",
    "for epoch in range(NEPOCHS):\n",
    "    loss = train_epoch(model, loader, criterion, optimizer)\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d} -> Train loss {1000*loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f593d-9dfb-4dc1-b058-89963d351383",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Performing an inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627aea6f-6d10-41c7-baec-7c0a60c352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set inference mode\n",
    "model.eval()\n",
    "\n",
    "## Validation dataset\n",
    "X_val = torch.rand((10,nfeatures))\n",
    "\n",
    "with torch.no_grad():\n",
    "    yp = model(X_val)\n",
    "yp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cec98-d386-44ad-8136-4d05cf3e0d5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5514d62-d759-479e-bafa-4182172c35ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The next table lists some hyperparameters used in this lecture:\n",
    "\n",
    "| Hyperparameter   | Description | Context | Value |\n",
    "| ---------------- | :---------: | :-----: | :---: |\n",
    "| _Number of layers_      | Deep the network    | Model Architecture | ? |\n",
    "| _Neurons per layer_     | Width of each layer | Model Architecture | ? |\n",
    "| _Activation functions_  | e.g., ReLU, sigmoid, tanh, ... | Model Architecture | ? |\n",
    "| _Learning rate_, $\\eta$ | Size of the update step            | Training | ? |\n",
    "| _Optimizer_  | Optimization algorithm: SGD, Adam, etc...     | Training | ? |\n",
    "| _Batch size_ | Samples processed for every update of weights | Training | ? |\n",
    "| _Number of epochs_ | Iterations over the entire dataset      | Training | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd42639-8a74-49ee-befc-b41823b17bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> &#9998; **Exercise:** <br>\n",
    "> Complete the __Value__ column in the table with the hyperparameters used in this section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "rise": {
   "scroll": true,
   "start_slideshow_at": "beginning",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
