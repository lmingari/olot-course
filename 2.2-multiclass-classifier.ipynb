{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc1ea-e5aa-407b-b062-102b2b2ed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install requirements\n",
    "from os.path import isfile\n",
    "\n",
    "repository   = \"https://github.com/lmingari/olot-course.git\"\n",
    "requirements = \"requirements-section2-2.txt\"\n",
    "\n",
    "if not isfile(requirements):\n",
    "    !git clone {repository}\n",
    "    %cd olot-course\n",
    "    !pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0ae62-871d-4244-9d38-16d01a926b69",
   "metadata": {},
   "source": [
    "# 2.2 A multilayer perceptron (MLP) for classification\n",
    "***\n",
    "\n",
    "* We propose a simple model for assessing the __impact of tephra fallout__ at a given location in La Palma based on deposit thickness data\n",
    "\n",
    "* We use the volcanic tephra deposition dataset from the 2021 Tajogaite Eruption on La Palma reported by [Shatto et al. (2024)][dataset]\n",
    "\n",
    "* The dataset comprises a total of 415 in-situ field measurements sampled across the Island of La Palma along with 66 values estimated from surface changes in areas not suitable for in-situ sampling\n",
    "\n",
    "* The dataset is available on [Zenodo][zenodo]\n",
    "\n",
    "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2352340923009800-gr2.jpg\" width=\"400\">\n",
    "\n",
    "[dataset]: https://doi.org/10.1016/j.dib.2023.109949\n",
    "[zenodo]: https://doi.org/10.5281/zenodo.8338991"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03df1c-c0cf-4275-a78c-c5be4c7617ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Definition of impact classes\n",
    "\n",
    "* We define three classes of tephra fallout impacts in terms of the deposit thickness:\n",
    "* __Goal:__ Train a MLP to \"predict\" the class of impact for a given coordinate (lat, lon)\n",
    "\n",
    "|Impact    | Thickness range   | Class |\n",
    "|----------|:-----------------:|:-----:|\n",
    "| Low      | < 1 mm            | 0     |\n",
    "| Moderate | 1 - 100 mm        | 1     |\n",
    "| High     | > 100 mm          | 2     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d7f34-2dbb-4755-af0e-d888e57bdaf3",
   "metadata": {},
   "source": [
    "## Main routines for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29e9ff-c827-4d0a-9bdd-8ba8c74731a5",
   "metadata": {},
   "source": [
    "#### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d14b9-5d96-4c25-be0e-001d55e79bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                        # tools for data manipulation and analysis\n",
    "import numpy as np                                         # numerical operations on arrays\n",
    "import matplotlib.pyplot as plt                            # plots and visualizations\n",
    "\n",
    "import torch                                               # main PyTorch library for tensor computation\n",
    "import torch.nn as nn                                      # building blocks for creating and training neural networks\n",
    "import torch.optim as optim                                # implementation of various optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader           # dataset utilities\n",
    "\n",
    "from sklearn.model_selection import train_test_split       # utility to easily split datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25b53a-f680-4e53-a6f1-495bf08d9fd6",
   "metadata": {},
   "source": [
    "#### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448ce9b-9736-45e6-9428-d3f9eb9f8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Performs one complete training epoch over the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to be trained\n",
    "                \n",
    "        loader: DataLoader that provides batches of training data. \n",
    "            Each iteration yields a batch of inputs and targets\n",
    "        \n",
    "        criterion: Loss function used to compute the training loss (e.g., nn.MSELoss).\n",
    "            Takes model predictions and targets as input\n",
    "        \n",
    "        optimizer: Optimization algorithm used to update model parameters (e.g., Adam, SGD).\n",
    "    \n",
    "    Returns:\n",
    "        Returns training metrics (average loss and accuracy for the epoch)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    corrects   = 0\n",
    "\n",
    "    # Mini-batch loop\n",
    "    for xb, yb in loader:\n",
    "        # Model prediction\n",
    "        logits = model(xb)\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Predict category\n",
    "        predicted_category = torch.argmax(logits,dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        corrects += (predicted_category == yb).sum().item()\n",
    "\n",
    "    # Return average loss and accuracy (%)\n",
    "    return total_loss / len(loader.dataset), 100 * corrects / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd42e1e-d97f-4c6c-9b23-c85b59786cea",
   "metadata": {},
   "source": [
    "#### Inference routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf5f2a-7e51-4015-a351-7bf3abbf2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, loader, criterion):\n",
    "    # Set inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            predicted_category = torch.argmax(logits,dim=1)\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            corrects += (predicted_category == yb).sum().item()\n",
    "\n",
    "    # Return average loss and accuracy (%)\n",
    "    return total_loss / len(loader.dataset), 100 * corrects / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b69399-f56b-4dee-b85b-9a3f9390488e",
   "metadata": {},
   "source": [
    "## Training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83bb2f-e5e8-48c7-8c8a-912d24b1187f",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be2192-2fe1-4f06-ae12-5f6faf7216b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/palma-deposit.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2847d-7a19-4767-b33f-42684e126958",
   "metadata": {},
   "source": [
    "#### Defining the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8eb203-d9b1-4a19-a038-b0276bf9f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the categories\n",
    "\n",
    "df['category'] = 0\n",
    "df.loc[df['thickness_cm']>0.1,'category'] = 1\n",
    "df.loc[df['thickness_cm']>10,'category']  = 2\n",
    "df\n",
    "#df['category'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a57985-bc19-4848-9304-c4cad61a2734",
   "metadata": {},
   "source": [
    "#### Plotting the distribution of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05ac0e-4488-44fd-a6aa-077fbd1d5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plot import create_map\n",
    "\n",
    "fig, ax = create_map()\n",
    "\n",
    "for target, label, color in [(0,'Low','b'), (1,'Moderate','y'), (2,'High','r')]:\n",
    "    df.loc[df.category==target].plot.scatter(\n",
    "        x='lon', y='lat', \n",
    "        c=color,\n",
    "        s=4,\n",
    "        alpha=0.5,\n",
    "        label = label,\n",
    "        ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17cad58-fd33-4991-bbf2-a43c374bad81",
   "metadata": {},
   "source": [
    "#### Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27290f20-785f-4e9c-8f41-e30a351433d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['lat','lon']\n",
    "X = df[features].values\n",
    "y = df['category'].values\n",
    "\n",
    "# Splitting arrays (80/20 %)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Size of the training dataset: \", X_train.shape)\n",
    "print(\"Size of the validation dataset: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb143ced-2594-4c71-aa07-84d2f3725a03",
   "metadata": {},
   "source": [
    "#### Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468689b7-02f3-43f9-8c25-81c4f959d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import Standardize, ThicknessDataset\n",
    "\n",
    "# Compute normalization stats from training data only\n",
    "mean = X_train.mean(axis=0)\n",
    "std  = X_train.std(axis=0)\n",
    "\n",
    "# Define transform\n",
    "transform = Standardize(mean,std)\n",
    "\n",
    "# Datasets\n",
    "dataset_train = ThicknessDataset(X_train, y_train, transform=transform)\n",
    "dataset_val   = ThicknessDataset(X_val, y_val, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0eca71-3b0b-4480-b28c-3ca010a40ef5",
   "metadata": {},
   "source": [
    "## Model arquitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235021fa-480b-4b4d-a633-d63748ce4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Simple MLP classifier\n",
    "# ---------------------------\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=64, out_dim=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)  # output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # shape [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f625c5e-fe44-4c99-966d-bb27269d264f",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ffdb3-0ce9-48d4-bd41-219f07b9e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Configuration\n",
    "#\n",
    "conf = {\n",
    "    'BATCH_SIZE': 16,\n",
    "    'LEARNING_RATE': 4E-4,\n",
    "    'NUM_EPOCHS': 400,\n",
    "}\n",
    "\n",
    "# DataLoader\n",
    "loader_train = DataLoader(dataset_train, batch_size=conf['BATCH_SIZE'], shuffle=True)\n",
    "loader_val   = DataLoader(dataset_val,   batch_size=conf['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = Classifier()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=conf['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11314500-12ee-407e-b585-f96fe72743e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs   = []\n",
    "val_losses   = []\n",
    "val_accs     = []\n",
    "\n",
    "for epoch in range(conf['NUM_EPOCHS']):\n",
    "    train_loss, train_acc = train_epoch(model, loader_train, criterion, optimizer)\n",
    "    val_loss,   val_acc   = evaluate_epoch(model, loader_val, criterion)\n",
    "\n",
    "    # Store current losses/accuracies\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Print log\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}:\")\n",
    "        print(f\"Train loss (accuracy): {train_loss:.4f} ({train_acc:.2f}%) || Validation loss (accuracy): {val_loss:.4f} ({val_acc:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd47bf-2ab5-4d39-98c0-94d1cf522b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 2, figsize=(12,5))\n",
    "\n",
    "axs[0].plot(train_losses, label = 'Training loss')\n",
    "axs[0].plot(val_losses,   label = 'Validation loss')\n",
    "\n",
    "axs[1].plot(train_accs, label = \"Training accuracy\")\n",
    "axs[1].plot(val_accs,   label = \"Validation accuracy\")\n",
    "\n",
    "axs[0].set(ylabel = 'Average loss', xlabel = 'Epoch')\n",
    "axs[1].set(ylabel = 'Accuracy (%)', xlabel = 'Epoch')\n",
    "\n",
    "for ax in axs: ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef62d4-02ad-4ef5-9573-79786cc68486",
   "metadata": {},
   "source": [
    "## Decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a010716-b1dd-412d-bbbb-350b64a12825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plot import plot_decision_regions\n",
    "\n",
    "fig, ax = plot_decision_regions(model, transform)\n",
    "\n",
    "for category, label, color in [(0,'Low','b'), (1,'Moderate','y'), (2,'High','r')]:\n",
    "    df.loc[df.category==category].plot.scatter(\n",
    "        x = 'lon', y = 'lat', \n",
    "        c     = color,\n",
    "        s     = 5,\n",
    "        alpha = 0.5,\n",
    "        label = label,\n",
    "        ax = ax)\n",
    "\n",
    "fig.set_size_inches(6, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e370f-6024-46a1-8bf5-5fb354bcf519",
   "metadata": {},
   "source": [
    "> &#9998; **Exercise:** <br>\n",
    "> * Redefine the model class `Classifier` by removing the ReLU activation functions\n",
    "> * How are the decision regions modified?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
