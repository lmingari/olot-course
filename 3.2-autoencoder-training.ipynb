{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c71652-7bd4-454c-bcc7-d3f496d02c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install requirements\n",
    "from os.path import isfile\n",
    "\n",
    "repository   = \"https://github.com/lmingari/olot-course.git\"\n",
    "requirements = \"requirements-section3-2.txt\"\n",
    "\n",
    "if not isfile(requirements):\n",
    "    !git clone {repository}\n",
    "    %cd olot-course\n",
    "    !pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204343e-2695-4671-b0c8-ec1cc1f5fe9c",
   "metadata": {},
   "source": [
    "# 3.2 Autoencoder (I): Training a CNN-based Autoencoder\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005acb7a-55d6-4861-8af0-83c4e48b57c2",
   "metadata": {},
   "source": [
    "## General configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e4ca2-ccff-4bca-b127-eb1e56db7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration ###\n",
    "config = {\n",
    "    'BATCH_SIZE':    16,\n",
    "    'LATENT_DIM':    2,\n",
    "    'LEARNING_RATE': 1E-3,\n",
    "    'NUM_EPOCHS':    150,\n",
    "    'RANDOM_SEED':   43,\n",
    "    'FNAME_MODEL':   'autoencoder.pt',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379c0bc-094f-4a39-9dfd-4620ff8c5d0a",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a525b-972a-473c-a8ee-bac04623a212",
   "metadata": {},
   "source": [
    "#### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cf761-06c3-4c2a-9d95-df87d1670761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03533a2a-d294-4c2c-ac53-a4e1be9ba8a5",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64c8e-c983-4c7e-a18f-d4940b16bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Performs one complete training epoch over the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to be trained\n",
    "                \n",
    "        loader: DataLoader that provides batches of training data.\n",
    "            Each iteration yields a batch of inputs\n",
    "        \n",
    "        criterion: Loss function used to compute the training loss (e.g., nn.MSELoss).\n",
    "            Takes model predictions and targets as input\n",
    "        \n",
    "        optimizer (torch.optim.Optimizer): Optimization algorithm used to update \n",
    "            model parameters (e.g., Adam, SGD)\n",
    "    \n",
    "    Returns:\n",
    "        Returns training metrics (average loss for the epoch).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Mini-batch loop\n",
    "    for batch in loader:\n",
    "        # Model prediction\n",
    "        prediction = model(batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(prediction,batch)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        current_batch_size = batch.size(0)\n",
    "        total_loss += loss.item()*current_batch_size\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad95b49-6853-4882-a8be-743031ebbba0",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04bcc0-d30e-4eb1-81e4-089573576696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, loader, criterion):\n",
    "    # Set inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Model prediction\n",
    "            prediction = model(batch)\n",
    "            # Compute loss\n",
    "            loss = criterion(prediction,batch)\n",
    "\n",
    "            # Update metrics\n",
    "            current_batch_size = batch.size(0)\n",
    "            total_loss += loss.item()*current_batch_size\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3b40b-17af-4cdb-81e1-236522a54cfc",
   "metadata": {},
   "source": [
    "## 1. Loading raw data and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce5ccd-a37e-40ad-82c5-2b17995acd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "\n",
    "## Get a FALL3D ensemble run output\n",
    "fname = \"data/tephra_col_mass.ens.nc\"\n",
    "if not isfile(fname):\n",
    "    !wget -P ./data https://saco.csic.es/s/wFpKYG5bHfTwKbi/download/tephra_col_mass.ens.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8343d57-6069-4809-b6d0-a12e59b9a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fname)\n",
    "da = ds[\"tephra_col_mass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147020af-f13f-4149-adac-669bf1b4e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Percentile-based scaling\n",
    "## Compute the 98th percentile for normalization\n",
    "print(f\"Maximum value: {da.max().item()}\")\n",
    "print(f\"98th percentile: {da.quantile(0.98).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7a24c-08df-47d5-b1a6-c491b3d22af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = da.plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7a2b5-d629-4c34-99cd-ed2bebebfbbf",
   "metadata": {},
   "source": [
    "## 2. Create a custom Dataset and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d5613-582a-4b7f-a448-cb463f7532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import EnsembleDataset, MinMaxScale\n",
    "\n",
    "## Re-scale between 0 and 20\n",
    "## so 98% of the data is between 0 and 1\n",
    "min_value = 0\n",
    "max_value = 20\n",
    "transform = MinMaxScale(min_value, max_value)\n",
    "\n",
    "## Create a Dataset object for the full dataset (training + validation)\n",
    "dataset = EnsembleDataset(da, transform)\n",
    "\n",
    "## Random split with in training and validation datasets\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.8 * n_total)   # 80% train\n",
    "n_val   = n_total - n_train    # 20% val\n",
    "\n",
    "torch.manual_seed(config['RANDOM_SEED'])\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee15d27-34d1-4a80-a56e-5ad857ecb12c",
   "metadata": {},
   "source": [
    "## 3. Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1b607-49f0-44fb-bc74-16cf039ca5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=config['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350371b-46e2-45f7-a59f-f0413992233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data by mini-batches with dimensions:\n",
    "## (nbatch,nchannels,nlat,nlon)\n",
    "for batch in train_loader:\n",
    "    print(\"Batch dimensions: (nbatch,nchannels,nlat,nlon)\")\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a472455-fc5f-475d-9511-a41e8eb6f35d",
   "metadata": {},
   "source": [
    "## 4. Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02c5f2-44e4-4a24-8ff1-97499dac008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import Autoencoder\n",
    "model = Autoencoder(config['LATENT_DIM'])\n",
    "summary(model, (1,101,121))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44792f4e-224d-4b2e-886a-b1d4595e7f1a",
   "metadata": {},
   "source": [
    "## 5. Loss function\n",
    "Creates a criterion that measures the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55abe6c-51cd-4773-9cbe-e8c86ad2b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e91463-0eb0-4ebe-88b8-b92ac389004f",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e0e7d-51b6-4ebd-8773-f08d72a04c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13447b14-f7a4-4dcb-9c2b-4b7886dc746f",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e75138-4fbe-4969-ba2e-5fc960453422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for every  epoch\n",
    "train_losses = []\n",
    "val_losses   = []\n",
    "\n",
    "for epoch in range(config['NUM_EPOCHS']):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss   = evaluate_epoch(model, val_loader, criterion)\n",
    "    # Store current losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if epoch%10 == 0 or epoch == config['NUM_EPOCHS']-1:\n",
    "        print(f\"Epoch {epoch+1:02d} -> Train loss {train_loss:.4f} | Validation loss: {val_loss:.4f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d132af-dd9e-4be6-808d-089be545c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = 'Training loss')\n",
    "plt.plot(val_losses, label = 'Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Averaged Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a8cbe-3395-418c-982b-a40d773cba4e",
   "metadata": {},
   "source": [
    "## State reconstruction using the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663007d-59c0-4feb-b831-b80577082195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "yp_list = [] # List of reconstructions\n",
    "xb_list = [] # List of original inputs\n",
    "\n",
    "## Iterate over the validation dataset\n",
    "with torch.no_grad():\n",
    "    for xb in val_loader:\n",
    "        prediction = model(xb)\n",
    "        yp = prediction.squeeze(1)\n",
    "        xb = xb.squeeze(1)\n",
    "        yp_list.append(transform.invert(yp))\n",
    "        xb_list.append(transform.invert(xb))\n",
    "    reconstructions = torch.cat(yp_list, dim=0)\n",
    "    inputs = torch.cat(xb_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d4496-79f2-4da9-86da-594737e38d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting reconstructions for validation dataset \n",
    "plot_conf = {\n",
    "    'cmap': 'RdYlBu_r',\n",
    "    'vmin': 0, \n",
    "    'vmax': 30,\n",
    "}\n",
    "\n",
    "n=min(12,n_val)\n",
    "fig, axs = plt.subplots(nrows = n, ncols = 2, figsize=(6,38))\n",
    "\n",
    "for i in range(n):\n",
    "    cs1=axs[i,0].pcolormesh(da.lon,da.lat,inputs[i], **plot_conf)\n",
    "    cs2=axs[i,1].pcolormesh(da.lon,da.lat,reconstructions[i], **plot_conf)\n",
    "                        \n",
    "for ax in axs.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "axs[0,0].set_title('Original model output')\n",
    "axs[0,1].set_title('Reconstructed model output')\n",
    "    \n",
    "cbar = fig.colorbar(cs2, \n",
    "             ax=axs, \n",
    "             orientation='horizontal',\n",
    "             fraction=0.05,\n",
    "             pad=0.02, \n",
    "             aspect=30\n",
    "            )\n",
    "cbar.set_label('Column mass [g/m2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ada565-ad82-41ff-a935-7bb2283f2843",
   "metadata": {},
   "source": [
    "## Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990675ae-5e11-44f1-9a00-df86db5cf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set(title = 'Latent space (z1,z2)', ylabel = 'z2', xlabel = 'z1')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        z = model.encode(batch)\n",
    "        ax.scatter(z[:,0],z[:,1], color='red')\n",
    "    for batch in val_loader:\n",
    "        z = model.encode(batch)\n",
    "        ax.scatter(z[:,0],z[:,1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56f4db-d916-4469-8b83-52c5436def61",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e09a79-4ee8-4e60-a293-1a86abd6b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),  # Trained Model parameters\n",
    "    'LATENT_DIM': config['LATENT_DIM'],      # Dimension of the latent space (=2)\n",
    "    'MINVAL': min_value,                     # Min value used for normalization (=0)\n",
    "    'MAXVAL': max_value,                     # Max value used for normalization (=20)\n",
    "    }, config['FNAME_MODEL'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
